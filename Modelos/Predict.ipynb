{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c896b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import shap\n",
    "from flask import Flask, request\n",
    "from io import StringIO\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733018a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df_consumo, df_temp):\n",
    "    df_consumo['date'] = pd.to_datetime(df_consumo['date'])\n",
    "    df_consumo = df_consumo[df_consumo['date'].dt.year == 2023]\n",
    "    df_consumo = df_consumo.sort_values(by='date')\n",
    "    df_consumo = df_consumo[df_consumo['date'].dt.year == datetime.now().year]\n",
    "    df_consumo = df_consumo.sort_values(by='date')\n",
    "    df_consumo = df_consumo.rename(columns={'volume': 'volumem3'})\n",
    "    df_consumo['volumem3'] = df_consumo['volumem3'].str.replace(' m3', '').astype(float)\n",
    "    df_consumo = df_consumo.rename(columns={'logVolume': 'logVolumem3'})\n",
    "    df_consumo['logVolumem3'] = df_consumo['logVolumem3'].str.replace(' m3', '').astype(float)\n",
    "    def clean_delta_volume(x):\n",
    "        if isinstance(x, list):\n",
    "            return [str(item).strip() for item in x]\n",
    "        elif isinstance(x, str):\n",
    "            return x.split(',')\n",
    "        else:\n",
    "            return [str(x)]\n",
    "\n",
    "    # Aplica la función 'clean_delta_volume' a cada elemento de la columna 'deltaVolumes'\n",
    "    df_consumo['deltaVolumes'] = df_consumo['deltaVolumes'].apply(clean_delta_volume)\n",
    "\n",
    "    # Calcula la longitud máxima de las listas en 'deltaVolumes'\n",
    "    max_length = df_consumo['deltaVolumes'].apply(len).max()\n",
    "    new_column_names = [f'deltaVolumes{i+1}cm3' for i in range(max_length)]\n",
    "    is_array = df_consumo['deltaVolumes'].apply(lambda x: isinstance(x, list))\n",
    "    df_consumo[new_column_names] = pd.DataFrame(df_consumo.loc[is_array, 'deltaVolumes'].values.tolist(), index=df_consumo.loc[is_array].index)\n",
    "\n",
    "    df_consumo[new_column_names] = df_consumo[new_column_names].apply(lambda x: x.str.replace(' m3', '').astype(float))\n",
    "    df_consumo.drop('deltaVolumes', axis=1, inplace=True)\n",
    "    df_consumo = df_consumo.drop(['state', 'stateMessages', 'error', 'logDate'], axis = 1)\n",
    "    # Crear un nuevo DataFrame vacío para almacenar los nuevos registros\n",
    "    new_df = pd.DataFrame(columns=['date', 'volumem3', 'logVolumem3', 'device_name', 'dev_eui', 'dev_addr', 'deltaVolumes'])\n",
    "\n",
    "    # Aplicar una función que crea los nuevos registros a partir de las columnas 'deltaVolumes'\n",
    "    def create_new_rows(row):\n",
    "        # Obtener los valores de las columnas que se deben conservar\n",
    "        date = row['date']\n",
    "        volumem3 = row['volumem3']\n",
    "        logVolumem3 = row['logVolumem3']\n",
    "        device_name = row['device_name']\n",
    "        dev_eui = row['dev_eui']\n",
    "        dev_addr = row['dev_addr']\n",
    "    \n",
    "        # Crear una lista vacía para almacenar los nuevos registros\n",
    "        new_rows = []\n",
    "    \n",
    "        # Iterar sobre las columnas 'deltaVolumes' en orden inverso\n",
    "        for i in range(15, 7, -1):\n",
    "            # Calcular la diferencia de horas respecto al registro original\n",
    "            hour_diff = 1 + i\n",
    "        \n",
    "            # Crear un nuevo registro con los valores correspondientes\n",
    "            new_row = {\n",
    "                'date': date + pd.DateOffset(hours=hour_diff),\n",
    "                'volumem3': volumem3,\n",
    "                'logVolumem3': logVolumem3,\n",
    "                'device_name': device_name,\n",
    "                'dev_eui': dev_eui,\n",
    "                'dev_addr': dev_addr,\n",
    "                'deltaVolumes': row[f'deltaVolumes{i}cm3']\n",
    "            }\n",
    "        \n",
    "            # Agregar el nuevo registro a la lista\n",
    "            new_rows.append(new_row)\n",
    "    \n",
    "        # Devolver la lista de nuevos registros\n",
    "        return new_rows\n",
    "\n",
    "    # Aplicar la función a cada fila del DataFrame original y obtener una lista de listas de nuevos registros\n",
    "    new_rows_list = df_consumo.apply(create_new_rows, axis=1)\n",
    "\n",
    "    # Convertir la lista de listas en una sola lista de nuevos registros\n",
    "    new_rows_list = [item for sublist in new_rows_list for item in sublist]\n",
    "\n",
    "    # Agregar los nuevos registros al DataFrame vacío\n",
    "    new_df = new_df.append(new_rows_list, ignore_index=True)\n",
    "\n",
    "    # Eliminar las columnas innecesarias del DataFrame original\n",
    "    df_consumo = df_consumo.drop(columns=[\n",
    "        'deltaVolumes1cm3', 'deltaVolumes2cm3', 'deltaVolumes3cm3',\n",
    "        'deltaVolumes4cm3', 'deltaVolumes5cm3', 'deltaVolumes6cm3',\n",
    "        'deltaVolumes7cm3', 'deltaVolumes8cm3', 'deltaVolumes9cm3',\n",
    "        'deltaVolumes10cm3', 'deltaVolumes11cm3', 'deltaVolumes12cm3',\n",
    "        'deltaVolumes13cm3', 'deltaVolumes14cm3', 'deltaVolumes15cm3'\n",
    "    ])\n",
    "\n",
    "    # Concatenar el DataFrame original con el nuevo DataFrame generado\n",
    "    df_consumo = pd.concat([df_consumo, new_df])\n",
    "\n",
    "    # Ordenar el DataFrame por fecha\n",
    "    df_consumo = df_consumo.sort_values('date')\n",
    "\n",
    "    # Restablecer los índices del DataFrame\n",
    "    df_consumo = df_consumo.dropna()\n",
    "    df_consumo = df_consumo.reset_index(drop=True)\n",
    "    # Eliminar las columnas 'volumem3' y 'logVolumem3'\n",
    "    df_consumo = df_consumo.drop(['volumem3', 'logVolumem3'], axis=1)\n",
    "\n",
    "    # Convertir la columna 'date' al formato de fecha adecuado\n",
    "    df_consumo['date'] = pd.to_datetime(df_consumo['date']).dt.date\n",
    "\n",
    "    # Sumar los valores de 'deltaVolumes' agrupados por 'device_name' y por día\n",
    "    df_con_sum = df_consumo.groupby(['date', 'device_name', 'dev_eui', 'dev_addr'])['deltaVolumes'].sum().reset_index()\n",
    "    df_con_sum = df_con_sum[['date', 'deltaVolumes']]\n",
    "\n",
    "    df_sum_date = df_con_sum.groupby('date', as_index=False).sum()\n",
    "    df_temp = df_temp.rename(columns={'fecha': 'date'})\n",
    "    df_temp = df_temp.drop(['provincia', 'indicativo', 'nombre', 'altitud'], axis = 1)\n",
    "    patron = r'[a-zA-Z]+'\n",
    "\n",
    "    df_temp.replace(to_replace=patron, value=np.nan, regex=True, inplace=True)\n",
    "    def convert_to_hour(value):\n",
    "        try:\n",
    "            return int(value.split(':', 1)[0])\n",
    "        except AttributeError:\n",
    "            return value\n",
    "\n",
    "    columns_to_convert = ['horaracha', 'horatmin', 'horatmax']\n",
    "\n",
    "    for column in columns_to_convert:\n",
    "        df_temp[column] = df_temp[column].apply(convert_to_hour)\n",
    "        \n",
    "    columnas_a_convertir = ['tmed', 'prec', 'tmin', 'tmax', 'dir',\n",
    "           'velmedia', 'racha', 'sol', 'presMax', 'presMin']\n",
    "\n",
    "    df_temp[columnas_a_convertir] = df_temp[columnas_a_convertir].astype(str).apply(lambda x: x.str.replace(',', '.')).astype(float)\n",
    "    aux_temp = df_temp[['tmed', 'prec', 'tmin', 'horatmin', 'tmax', 'horatmax', 'dir',\n",
    "       'velmedia', 'racha', 'horaracha', 'sol', 'presMax', 'horaPresMax',\n",
    "       'presMin', 'horaPresMin']]\n",
    "    # Lista de columnas con NaN en el DataFrame original\n",
    "    cols_with_nan = aux_temp.columns[aux_temp.isnull().any()].tolist()\n",
    "\n",
    "    # Iterar por cada columna con NaN para predecir sus valores\n",
    "    for col in cols_with_nan:\n",
    "        # Separamos los registros con valores conocidos y desconocidos para esta columna\n",
    "        df_known = aux_temp.dropna(subset=[col])\n",
    "        df_unknown = aux_temp[aux_temp[col].isnull()]\n",
    "\n",
    "        # Creamos el modelo de regresión lineal\n",
    "        regression_model = LinearRegression()\n",
    "\n",
    "        # Ajustamos el modelo con los datos conocidos para esta columna\n",
    "        X = df_known.drop(columns=cols_with_nan).values\n",
    "        y = df_known[col].values\n",
    "        regression_model.fit(X, y)\n",
    "\n",
    "        # Hacemos las predicciones para los valores desconocidos en esta columna\n",
    "        X_unknown = df_unknown.drop(columns=cols_with_nan).values\n",
    "        predicted_values = regression_model.predict(X_unknown)\n",
    "\n",
    "        # Convertimos los valores predichos a enteros sin decimales\n",
    "        predicted_values = predicted_values.astype(int)\n",
    "\n",
    "        # Rellenamos los valores faltantes en el DataFrame original para esta columna\n",
    "        aux_temp.loc[aux_temp[col].isnull(), col] = predicted_values\n",
    "    df_tiemp = pd.concat([df_temp['date'], aux_temp], axis=1)\n",
    "    df_sum_date['date'] = pd.to_datetime(df_sum_date['date'])\n",
    "    df_tiemp['date'] = pd.to_datetime(df_tiemp['date'])\n",
    "    df_merged = pd.merge(df_sum_date, df_tiemp, on='date', how='inner')\n",
    "    df_merged['NumMes'] = df_merged['date'].dt.month\n",
    "    # Crea una columna para el día de la semana (1-7, donde 1 es Lunes y 7 es Domingo)\n",
    "    df_merged['DiaSem'] = df_merged['date'].dt.dayofweek + 1\n",
    "    df_merged = df_merged[['NumMes', 'DiaSem'] + [col for col in df_merged.columns if col not in ['NumMes', 'DiaMes']]]\n",
    "    df_merged = df_merged.drop(columns=['date'])\n",
    "    df_merged['previous_deltaVolumes'] = df_merged['deltaVolumes'].shift(1)\n",
    "    df_merged.dropna(inplace=True)\n",
    "    column_names = df_merged.columns.tolist()\n",
    "\n",
    "    if \"deltaVolumes\" in column_names:\n",
    "        column_names.remove(\"deltaVolumes\")\n",
    "\n",
    "    column_names.append(\"deltaVolumes\")\n",
    "\n",
    "    df_merged = df_merged[column_names]\n",
    "    df_merged['deltaVolumes'] = df_merged['deltaVolumes'].astype(float)\n",
    "    df_merged['horaPresMax'] = df_merged['horaPresMax'].astype(float)\n",
    "    df_merged['horaPresMin'] = df_merged['horaPresMin'].astype(float)\n",
    "    # Separar las características y la variable objetivo\n",
    "    X = df_merged[['NumMes', 'tmed', 'tmin', 'tmax',\n",
    "       'horatmax', 'dir', 'velmedia', 'racha', 'sol', 'presMax',\n",
    "       'horaPresMax', 'presMin', 'previous_deltaVolumes']]\n",
    "\n",
    "    y = df_merged['deltaVolumes']\n",
    "\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    df_Rand = RandomForestRegressor(n_estimators=50, max_depth=20, random_state=10)\n",
    "    df_Rand.fit(X_train, y_train)\n",
    "    val_Rand_predictions = df_Rand.predict(X_test)\n",
    "    \n",
    "    explainer = shap.Explainer(df_Rand, X_train)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.summary_plot(shap_values, X_test)\n",
    "    shap.initjs()\n",
    "    shap.force_plot(explainer.expected_value, shap_values, X_test)\n",
    "    shap.force_plot(explainer.expected_value, shap_values[0, :], X.iloc[0, :])\n",
    "    \n",
    "    return val_Rand_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(df_consumo, df_consumo_hist):\n",
    "    df_consumo \n",
    "    df_consumo['date'] = pd.to_datetime(df_consumo['date'])\n",
    "    df_consumo = df_consumo[df_consumo['date'].dt.year == 2023]\n",
    "    df_consumo = df_consumo.sort_values(by='date')\n",
    "    df_consumo = df_consumo[df_consumo['date'].dt.year == datetime.now().year]\n",
    "    df_consumo = df_consumo.sort_values(by='date')\n",
    "    df_consumo = df_consumo.rename(columns={'volume': 'volumem3'})\n",
    "    df_consumo['volumem3'] = df_consumo['volumem3'].str.replace(' m3', '').astype(float)\n",
    "    df_consumo = df_consumo.rename(columns={'logVolume': 'logVolumem3'})\n",
    "    df_consumo['logVolumem3'] = df_consumo['logVolumem3'].str.replace(' m3', '').astype(float)\n",
    "    def clean_delta_volume(x):\n",
    "        if isinstance(x, list):\n",
    "            return [str(item).strip() for item in x]\n",
    "        elif isinstance(x, str):\n",
    "            return x.split(',')\n",
    "        else:\n",
    "            return [str(x)]\n",
    "\n",
    "    # Aplica la función 'clean_delta_volume' a cada elemento de la columna 'deltaVolumes'\n",
    "    df_consumo['deltaVolumes'] = df_consumo['deltaVolumes'].apply(clean_delta_volume)\n",
    "\n",
    "    # Calcula la longitud máxima de las listas en 'deltaVolumes'\n",
    "    max_length = df_consumo['deltaVolumes'].apply(len).max()\n",
    "    new_column_names = [f'deltaVolumes{i+1}cm3' for i in range(max_length)]\n",
    "    is_array = df_consumo['deltaVolumes'].apply(lambda x: isinstance(x, list))\n",
    "    df_consumo[new_column_names] = pd.DataFrame(df_consumo.loc[is_array, 'deltaVolumes'].values.tolist(), index=df_consumo.loc[is_array].index)\n",
    "\n",
    "    df_consumo[new_column_names] = df_consumo[new_column_names].apply(lambda x: x.str.replace(' m3', '').astype(float))\n",
    "    df_consumo.drop('deltaVolumes', axis=1, inplace=True)\n",
    "    df_consumo = df_consumo.drop(['state', 'stateMessages', 'error', 'logDate'], axis = 1)\n",
    "    # Crear un nuevo DataFrame vacío para almacenar los nuevos registros\n",
    "    new_df = pd.DataFrame(columns=['date', 'volumem3', 'logVolumem3', 'device_name', 'dev_eui', 'dev_addr', 'deltaVolumes'])\n",
    "\n",
    "    # Aplicar una función que crea los nuevos registros a partir de las columnas 'deltaVolumes'\n",
    "    def create_new_rows(row):\n",
    "        # Obtener los valores de las columnas que se deben conservar\n",
    "        date = row['date']\n",
    "        volumem3 = row['volumem3']\n",
    "        logVolumem3 = row['logVolumem3']\n",
    "        device_name = row['device_name']\n",
    "        dev_eui = row['dev_eui']\n",
    "        dev_addr = row['dev_addr']\n",
    "    \n",
    "        # Crear una lista vacía para almacenar los nuevos registros\n",
    "        new_rows = []\n",
    "    \n",
    "        # Iterar sobre las columnas 'deltaVolumes' en orden inverso\n",
    "        for i in range(15, 7, -1):\n",
    "            # Calcular la diferencia de horas respecto al registro original\n",
    "            hour_diff = 1 + i\n",
    "        \n",
    "            # Crear un nuevo registro con los valores correspondientes\n",
    "            new_row = {\n",
    "                'date': date + pd.DateOffset(hours=hour_diff),\n",
    "                'volumem3': volumem3,\n",
    "                'logVolumem3': logVolumem3,\n",
    "                'device_name': device_name,\n",
    "                'dev_eui': dev_eui,\n",
    "                'dev_addr': dev_addr,\n",
    "                'deltaVolumes': row[f'deltaVolumes{i}cm3']\n",
    "            }\n",
    "        \n",
    "            # Agregar el nuevo registro a la lista\n",
    "            new_rows.append(new_row)\n",
    "    \n",
    "        # Devolver la lista de nuevos registros\n",
    "        return new_rows\n",
    "\n",
    "    # Aplicar la función a cada fila del DataFrame original y obtener una lista de listas de nuevos registros\n",
    "    new_rows_list = df_consumo.apply(create_new_rows, axis=1)\n",
    "\n",
    "    # Convertir la lista de listas en una sola lista de nuevos registros\n",
    "    new_rows_list = [item for sublist in new_rows_list for item in sublist]\n",
    "\n",
    "    # Agregar los nuevos registros al DataFrame vacío\n",
    "    new_df = new_df.append(new_rows_list, ignore_index=True)\n",
    "\n",
    "    # Eliminar las columnas innecesarias del DataFrame original\n",
    "    df_consumo = df_consumo.drop(columns=[\n",
    "        'deltaVolumes1cm3', 'deltaVolumes2cm3', 'deltaVolumes3cm3',\n",
    "        'deltaVolumes4cm3', 'deltaVolumes5cm3', 'deltaVolumes6cm3',\n",
    "        'deltaVolumes7cm3', 'deltaVolumes8cm3', 'deltaVolumes9cm3',\n",
    "        'deltaVolumes10cm3', 'deltaVolumes11cm3', 'deltaVolumes12cm3',\n",
    "        'deltaVolumes13cm3', 'deltaVolumes14cm3', 'deltaVolumes15cm3'\n",
    "    ])\n",
    "\n",
    "    # Concatenar el DataFrame original con el nuevo DataFrame generado\n",
    "    df_consumo = pd.concat([df_consumo, new_df])\n",
    "\n",
    "    # Ordenar el DataFrame por fecha\n",
    "    df_consumo = df_consumo.sort_values('date')\n",
    "\n",
    "    # Restablecer los índices del DataFrame\n",
    "    df_consumo = df_consumo.dropna()\n",
    "    df_consumo = df_consumo.reset_index(drop=True)\n",
    "    # Eliminar las columnas 'volumem3' y 'logVolumem3'\n",
    "    df_consumo = df_consumo.drop(['volumem3', 'logVolumem3'], axis=1)\n",
    "\n",
    "    # Convertir la columna 'date' al formato de fecha adecuado\n",
    "    df_consumo['date'] = pd.to_datetime(df_consumo['date']).dt.date\n",
    "\n",
    "    # Sumar los valores de 'deltaVolumes' agrupados por 'device_name' y por día\n",
    "    df_con_sum = df_consumo.groupby(['date', 'device_name', 'dev_eui', 'dev_addr'])['deltaVolumes'].sum().reset_index()\n",
    "    df_con_sum = df_con_sum[['date', 'deltaVolumes']]\n",
    "\n",
    "    df_sum_date = df_con_sum.groupby('date', as_index=False).sum()\n",
    "\n",
    "    df_sum_date = df_sum_date.rename(columns={'date': 'Fecha', 'deltaVolumes': 'Valor'})\n",
    "\n",
    "    df_sum_date['Fecha'] = pd.to_datetime(df_sum_date['Fecha'])\n",
    "    df_consumo_hist = df_consumo_hist.drop(columns=['Unidad'])\n",
    "    df_consumo_hist['Fecha'] = pd.to_datetime(df_consumo_hist['Fecha']).dt.date\n",
    "    df_consumo_hist['Fecha'] = pd.to_datetime(df_consumo_hist['Fecha'])\n",
    "    df_consumo_hist['Valor'] = df_consumo_hist['Valor'].astype(float)\n",
    "    df_datos = pd.concat([df_consumo_hist, df_sum_date], ignore_index=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    df_datos['Valor'] = scaler.fit_transform(df_datos[['Valor']])\n",
    "    df_datos = df_datos.sort_values(by=\"Fecha\")\n",
    "    df_datos = df_datos.reset_index(drop=True)\n",
    "    # Función para crear secuencias de datos\n",
    "    def crear_secuencias(data, pasos):\n",
    "        secuencias_x, secuencias_y = [], []\n",
    "        for i in range(len(data) - pasos):\n",
    "            secuencias_x.append(data[i:i+pasos])\n",
    "            secuencias_y.append(data[i+pasos])\n",
    "        return np.array(secuencias_x), np.array(secuencias_y)\n",
    "\n",
    "    # Definir el número de pasos de tiempo (cuántos valores pasados se usarán para predecir el futuro)\n",
    "    num_pasos = 10\n",
    "    X, y = crear_secuencias(df_datos['Valor'].values, num_pasos)\n",
    "\n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    porcentaje_entrenamiento = 0.8\n",
    "    indice_entrenamiento = int(porcentaje_entrenamiento * len(X))\n",
    "\n",
    "    X_entrenamiento, X_prueba = X[:indice_entrenamiento], X[indice_entrenamiento:]\n",
    "    y_entrenamiento, y_prueba = y[:indice_entrenamiento], y[indice_entrenamiento:]\n",
    "\n",
    "    # Reformatear los datos para que sean 3D (número de muestras, pasos de tiempo, número de características)\n",
    "    X_entrenamiento = X_entrenamiento.reshape(-1, num_pasos, 1)\n",
    "    X_prueba = X_prueba.reshape(-1, num_pasos, 1)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(num_pasos, 1)))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_entrenamiento, y_entrenamiento, epochs=100, batch_size=32, verbose=1)\n",
    "    # Evaluar el modelo en los datos de prueba\n",
    "    loss = model.evaluate(X_prueba, y_prueba)\n",
    "\n",
    "    # Realizar predicciones\n",
    "    predicciones = model.predict(X_prueba)\n",
    "    predicciones_originales = scaler.inverse_transform(predicciones)\n",
    "    \n",
    "    # Invertir la normalización en los valores reales de prueba\n",
    "    y_prueba_originales = scaler.inverse_transform(y_prueba.reshape(-1, 1))\n",
    "\n",
    "    # Obtener las fechas correspondientes a los datos de prueba\n",
    "    fechas_prueba = df_datos['Fecha'].values[-len(y_prueba_originales):]\n",
    "\n",
    "    # Plotear las fechas, valores reales y predicciones\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(fechas_prueba, y_prueba_originales, label='Valores reales', marker='o')\n",
    "    plt.plot(fechas_prueba, predicciones_originales, label='Predicciones', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.title('Predicciones vs. Valores reales con Fechas')\n",
    "    plt.xticks(rotation=45)  # Rotar las etiquetas del eje x para una mejor visualización\n",
    "    plt.show()\n",
    "    \n",
    "    return predicciones_originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf39077b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7cb8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
